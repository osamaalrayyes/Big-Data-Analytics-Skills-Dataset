{"cells":[{"cell_type":"code","source":["# Setup:\n\n## The following code will load the necessary PySpark libraries.\n\n# The following code will load PySpark and start a spark session.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# The following code will load the SQL function with the alias \"SQL\".\nfrom pyspark.sql import functions as SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4e1f104b-93dc-4806-b02d-ae4237b7aa62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Data Cleaning and Preparation:\n\n## The following code will read the files uploaded as tables into data frames.\n\n# This function will read the \"skill2vec_50K.csv.gz\" file as a data frame without headers.\ndf = spark.read.csv(\"/FileStore/tables/skill2vec_50K.csv.gz\", header = False)\n\n# This function will read the \"Technology_Skills.txt\" file as a data frame with tab as the separator and the first row as the header.\ndf_text = spark.read.csv(\"/FileStore/tables/Technology_Skills.txt\", sep = \"\\t\", header = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fa104e5-cc2a-4ec6-9574-f8dd1f868497","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Q1)\n\n## The following code will extract the amount of distinct job descriptions count using PySpark's SQL count distinct function.\n\nfrom pyspark.sql.functions import countDistinct, col\n\nDistinctCount = df.agg(countDistinct(col(df.columns[0])).alias(\"Distinct Job Descriptions Count\"))\n\nDistinctCount.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"08738d0b-7d95-4772-9d09-af945c0afc78","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------------+\n|Distinct Job Descriptions Count|\n+-------------------------------+\n|                          50000|\n+-------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Q2)\n\n## The following code will find the frequency of each skill and show the top 10 skill frequencies using PySpark's map reduce functions.\n\n# The data frame will be transformed into an RDD of skills using the flatMap function by applying a function to each value in the RDD and then flattening the results.\n# The lambda function will iterate over each row and extract the skills while also filtering out missing values. The set function will filter out duplicate skills.\nskills_rdd = df.rdd.flatMap(lambda row: set([skill for skill in row[1:] if skill is not None]))\n\n# This function will map each skill to a tuple of (skill, 1) for counting.\ncount_rdd = skills_rdd.map(lambda skill: (skill, 1))\n\n# This function will reduce the tuples by skill to get the total frequency of each skill.\nfreq = count_rdd.reduceByKey(lambda x, y: x + y)\n\n# The following function will convert the resulting RDD back into a data frame with the columns \"Skill\" and \"Freq\" sorted in descending order of  frequency of skills.\nfrequency = freq.toDF([\"Skill\", \"Freq\"]).orderBy(\"Freq\", ascending = False)\n\n#The following function will show the top 10 results.\nfrequency.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"92ffd33d-df43-4d48-b757-c3e343666c78","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+----+\n|               Skill|Freq|\n+--------------------+----+\n|                Java|1911|\n|          Javascript|1770|\n|               Sales|1705|\n|Business Development|1545|\n|    Web Technologies|1313|\n|Communication Skills|1305|\n|         development|1238|\n|           Marketing|1184|\n|             Finance|1078|\n|                HTML|1067|\n+--------------------+----+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Q3)\n\n## The following code will find the count of skills in each job descriptions and then show the most 5 occuring count of skills using PySpark's map reduce functions.\n\nfrom pyspark.sql.functions import desc\n\n# This function will transform the data frame into an RDD of tuples with job descriptions and a list of skills.\nrdd = df.rdd.map(lambda x: (x[0], x[1:]))\n\n# This code will define a map function that extracts the job description, counts the number of unique skills for the job, and returns a list of tuples with job descriptions, count, and skill.\ndef map_fun(row):\n    job_descriptions, skills = row\n    counted_skills = set()\n    unique_skills = []\n    for skill in skills: \n        if skill and skill not in counted_skills:\n            counted_skills.add(skill)\n            unique_skills.append(skill)\n    count = len(unique_skills)\n    return [(job_descriptions, count, skill) for skill in unique_skills]\n\n# This code will define a reduce function that sums the skill counts for each job description.\ndef reduce_fun(x, y): return (x[0], x[1] + y[1], x[2] + y[2])\n\n# This function will apply the map and reduce functions to the RDD to get a flattened RDD with job descriptions, skill count, and skill.\nskill_count_rdd = rdd.flatMap(lambda x: map_fun(x)).map(lambda x: ((x[0], x[1]), x[2])).reduceByKey(lambda x, y: x + ', ' + y).map(lambda x: (x[0][0], x[0][1]))\n\n# This function will convert the RDD back into a data frame with columns \"Job Description\" and \"Skill Count\", ordered by skill count in descending order.\nskill_count_df = skill_count_rdd.toDF([\"Job Description\", \"Skill Count\"]).orderBy(\"Skill Count\", ascending = False)\n\n# This code will group the data frame by skill count and count the number of occurrences.\nskill_count_occurrence = skill_count_df.groupBy(\"Skill Count\").count()\n\n# This code will order the resulting data frame by skill count frequency in descending order and show the top 5 most frequent skill counts.\nskill_count_occurrence = skill_count_occurrence.orderBy(desc(\"count\"))\nskill_count_occurrence.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"509165b3-aa49-450a-b236-4dce8f91e387","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+-----+\n|Skill Count|count|\n+-----------+-----+\n|         10|10477|\n|          5| 3432|\n|          6| 3405|\n|          1| 3386|\n|          7| 3345|\n+-----------+-----+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Q4)\n\n## The following code will find the frequency of each skill and show the top 10 skill frequencies using PySpark's map reduce functions after lowercasing the skills.\n\nfrom pyspark.sql.functions import lower\n\n# This function will apply the lower function to all columns in the data frame that are not missing and create a new data frame with the updated values.\nlowercase_df = df.select([lower(col).alias(col) if col in df.columns else col for col in df.columns])\n\n# The data frame will be transformed into an RDD if skills using the flatMap function by applying a function to each value in the RDD and then flattening the results.\n# The lambda function will iterate over each row and extract the skills by filtering out missing values.\nskills_rdd_lowercase = lowercase_df.rdd.flatMap(lambda row: [skill for skill in row[1:] if skill is not None])\n\n# This function will map each skill to a tuple of (skill, 1) for counting.\nskills_rdd_lowercase_map = skills_rdd_lowercase.map(lambda skill: (skill, 1))\n\n# This function will reduce the tuples by skill to get the total frequency of each skill.\nskills_rdd_lowercase_reduce = skills_rdd_lowercase_map.reduceByKey(lambda x, y: x + y)\n\n# This code will group the lowercased data frame by skill count and count the number of occurrences.\nlowercase_frequency = skills_rdd_lowercase_reduce.toDF([\"Skill\", \"Freq\"]).orderBy(\"Freq\", ascending = False)\n\n# The following function will show the top 10 results.\nlowercase_frequency.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"08502934-a4bd-4fe2-9adf-e26f2fb1cdeb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+----+\n|               Skill|Freq|\n+--------------------+----+\n|                java|2759|\n|          javascript|2738|\n|               sales|2680|\n|business development|2108|\n|           marketing|1809|\n|                 sql|1564|\n|              jquery|1547|\n|                html|1540|\n|communication skills|1539|\n|                 bpo|1530|\n+--------------------+----+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Q5)\n\n## The following code will use PySpark's SQL functions to remove the non-null and duplicate skills, lowercase the data frames, join the data frames by the skills, and find the skill count before and after joint. (The questions will be divided into 3 parts to ease interpretation.)\n\n\n## Part A: Transforming the dataset to find the count of skills in each row.\n\n\n# This function will get a list of column names from the data frame.\nskills = df.columns\n\n# This function will create a new data frame with an array column containing the list of skills for each row.\nskills_array = df.select(SQL.array(skills).alias(\"skills\"))\n\n# This function will add a new column to the data frame with the number of skills in the skills array.\nskills_array_count = skills_array.withColumn(\"skills_freq\", SQL.size(\"skills\"))\n \n# This function will add a new column to the data frame with an array of non-null skills for each row.\nskills_array_count_nonNA = skills_array_count.withColumn(\"skills_nonNA\", SQL.array_except(\"skills\", SQL.array(SQL.lit(None))))\n \n# This function will add a new column to the data frame with the total number of non-null skills for each row.\nskills_array_count_nonNA_frequency = skills_array_count_nonNA.withColumn(\"skills_freq_nonNA\", SQL.size(SQL.col(\"skills_nonNA\")))\n\n\n## Part B: Removing duplicate values from the rows and finding the initial skill count after lower-casing.\n \n    \n# This function will define a UDF that removes the first element of an array.\nremove = SQL.udf(lambda x: x[1:], SQL.ArrayType(SQL.StringType()))\n \n# This function will remove the first element of each skills_nonNA array and update the data frame.\nfrequency = skills_array_count_nonNA_frequency.withColumn('skills_nonNA', remove(SQL.col('skills_nonNA')))\n\n# This function will add a new column to the data frame with the count of skills in the skills_nonNA array for each row.\nfrequency_df = frequency.withColumn(\"skills_freq\", SQL.size(\"skills_nonNA\"))\n\n# This function will create a new data frame with the 'skills' column exploded into separate rows.\nfrequency_df_exploded = frequency_df.withColumn(\"skills\", SQL.explode(SQL.col( \"skills_nonNA\" )))\n\n# This function will select only the 'skills' column from the 'frequency_df_explode' data frame.\nfrequency_count_df = frequency_df_exploded.select(SQL.col(\"skills\"))\n\n# This function will create a new data frame with the 'skills' column in lower case.\nskills_count_lowered = frequency_count_df.select(SQL.lower(frequency_count_df.skills).alias(\"skills_lowered\"))\n\n# This function will count the number of rows in the 'skills_count_lowered' data frame.\ncount_skills = skills_count_lowered.count()\n\n\n## Part C: Lowercasing the new data frame and joining the data frames on skills to find the count after join.\n\n\n# This function will create a new data frame with the 'Example' column in lower case.\ndf_text_lowered = df_text.select(\"*\", SQL.lower(df_text.Example).alias(\"skills_lowered\"))\n \n# This function will join the 'skills_count_lowered' and 'df_text_lowered' data frames on the 'skills_lowered' column with an inner join.\njoint_df = skills_count_lowered.join(df_text_lowered, df_text_lowered[\"skills_lowered\"] == skills_count_lowered[\"skills_lowered\"], \"inner\")\n\n# This function will count the number of rows in the 'joint_df' data frame.\njoint_sum_of_skill_counts = joint_df.count()\n \n# This function will print the count of skills before and after the join.\nprint(\"The count of skills before the joint is\", count_skills, \", and after the join it becomes\", joint_sum_of_skill_counts, \".\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"78df01a8-a2d9-42e9-b9ab-2617f395ca60","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["The count of skills before the joint is 463803 , and after the join it becomes 1101498 .\n"]}],"execution_count":0},{"cell_type":"code","source":["#Q6)\n\n## The following code will find the total distinct skill count grouped by commodity tittle and show the 10 highest results using PySpark SQL's select, count and sort functions.\njoint_df_commodity = joint_df.select(SQL.col(\"Commodity Title\"))\njoint_df_commodity.groupby(\"Commodity Title\").count().sort(\"count\", ascending = False).show(truncate = False, n = 10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7389ef01-fa1f-4e32-8ea9-93f69c9b3508","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------------------------------+------+\n|Commodity Title                                  |count |\n+-------------------------------------------------+------+\n|Object or component oriented development software|324521|\n|Web platform development software                |298754|\n|Operating system software                        |190926|\n|Development environment software                 |53013 |\n|Data base management system software             |44132 |\n|Analytical or scientific software                |33552 |\n|Web page creation and editing software           |31682 |\n|Data base user interface and query software      |29436 |\n|Spreadsheet software                             |18568 |\n|File versioning software                         |13846 |\n+-------------------------------------------------+------+\nonly showing top 10 rows\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Coursework Big Data Analytics","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
